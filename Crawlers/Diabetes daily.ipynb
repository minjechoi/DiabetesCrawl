{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for crawling Diabetes daily\n",
    "- started with Type-1 diabetes forum\n",
    "- experienced occasional timing-out (unobserved in other communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# import requests, re, csv\n",
    "# import urllib\n",
    "# from urllib.request import Request,urlopen\n",
    "# from TorCrawler import TorCrawler\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from fake_useragent import UserAgent\n",
    "from lxml.html import fromstring\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I got too bothered to create a proper code for obtaining the pages of each forum and instead \n",
    "dumped all of them into this list    \n",
    "\"\"\"\n",
    "base_list = [\n",
    "    ('feedback-forum',63),\n",
    "    ('announcements', 19),\n",
    "    ('diabetes-news-and-studies', 202),\n",
    "    ('personal-updates', 45),\n",
    "    ('type-1-diabetes', 551),\n",
    "    ('type-1-5-diabetes', 40),\n",
    "    ('type-2-diabetes', 1078),\n",
    "    ('pre-diabetes', 104),\n",
    "    ('pregnancy', 28),\n",
    "    ('friends-and-family', 20),\n",
    "    ('food-and-diet', 248),\n",
    "    ('low-carb-diet', 81),\n",
    "    ('recipes', 110),\n",
    "    ('testing-blood-sugar', 154),\n",
    "    ('multiple-daily-injections-mdi-', 47),\n",
    "    ('insulin-pumps', 156),\n",
    "    ('continuous-glucose-monitors-cgms-', 41),\n",
    "    ('exercise', 66),\n",
    "    ('weight-loss', 33),\n",
    "    ('depression-and-staying-positive', 56),\n",
    "    ('eyes', 24),\n",
    "    ('kidneys', 10),\n",
    "    ('skin-wound-and-infections', 2),\n",
    "    ('heart', 7),\n",
    "    ('neuropathy', 22),\n",
    "    ('complications', 25),\n",
    "    ('women-s-corner', 12),\n",
    "    ('men-s-corner', 8),\n",
    "    ('non-traditional-treatments', 21),\n",
    "    ('united-states', 28),\n",
    "    ('united-kingdom', 2),\n",
    "    ('canada', 3),\n",
    "    ('australia', 2),\n",
    "    ('arts-and-culture', 7),\n",
    "    ('research-and-clinical-trials', 26),\n",
    "    ('other-medical-conditions', 48),\n",
    "    ('promotions-surveys-and-trial-recruitment', 25),\n",
    "    ('there-s-more-to-life-than-diabetes-', 273),\n",
    "    ('humor', 106),\n",
    "    ('non-diabetes-news-and-links', 32),\n",
    "    ('diabetes-daily-meetups', 16),\n",
    "    ('book-club', 9)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. open url with random agent\n",
    "\"\"\"\n",
    "1. Enters each forum, starts from the 1st page\n",
    "2. For each page, crawls the\n",
    "    (title, post address, username, user address(URL), date, number of replies, number of views)\n",
    "    of each post\n",
    "3. After each forum is completed, the data is stored in /data/site_name/forum_name/users.pckl as a list\n",
    "\"\"\"\n",
    "\n",
    "for k,(forum,NUM_PAGES) in enumerate(base_list):\n",
    "    print('[%d] Crawling %s...' %(k+1,forum))\n",
    "    start = time.time()\n",
    "    base_url = 'https://www.diabetesdaily.com/forum/%s/' %forum\n",
    "    if not os.path.exists(os.path.join('../data/diabetes_daily',forum)):\n",
    "        os.mkdir(os.path.join('diabetes_daily',forum))\n",
    "\n",
    "    outputs = []\n",
    "    for page_num in range(1,NUM_PAGES+1):\n",
    "        url = base_url + 'index%d?daysprune=-1'%(page_num)\n",
    "        # start a request with a random agent\n",
    "        sleep(np.random.randint(3,15))\n",
    "        req = requests.get(url=url,headers={'User-Agent':ua.random})\n",
    "        soup = BS(req.text,'lxml')\n",
    "        S1 = soup.find('ol','threads')\n",
    "        for S2 in S1.find_all(recursive=False):\n",
    "            if 'moved' in S2['class']:\n",
    "                continue\n",
    "            else:\n",
    "                title = S2.find('a','title').text.strip()\n",
    "                post_addr = S2.find('a','title')['href'][len(base_url):-1].split('/')\n",
    "                post_addr = '/'.join(post_addr[:-1])\n",
    "                user_name = S2.find('a','username understate').text.strip()\n",
    "                user_addr = S2.find('a','username understate')['href'].split('/')[-2]\n",
    "                date_time = S2.find('a','username understate')['title'].split(' on ')[-1]\n",
    "                scores = S2.find('ul','threadstats td alt').find_all(recursive=False)\n",
    "                replies = int(scores[0].text.replace(',','').strip().split('\\t')[-1])\n",
    "                views = int(scores[1].text.replace(',','').strip().split(' ')[-1])\n",
    "                outputs.append((title,post_addr,user_name,user_addr,date_time,replies,views))\n",
    "        if (page_num)%50==0:\n",
    "            print(\"Taking a 30 second rest at page %d...\"%(page_num))\n",
    "            sleep(30)\n",
    "        with open('../data/diabetes_daily/%s/users.pckl'%forum,'wb') as f:\n",
    "            pickle.dump(outputs,f)\n",
    "    print('[%d] Crawled %s! %d users crawled' %(k+1,forum,len(outputs)))\n",
    "    print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: for obtaining the data of a single instance\n",
    "# base_url = 'https://www.diabetesdaily.com/forum/introduce-yourself/'\n",
    "# url = base_url+'index11?daysprune=-1' %\n",
    "# req = requests.get(url=url,headers={'User-Agent':ua.random})\n",
    "# soup = BS(req.text,'lxml')\n",
    "# S1 = soup.find('ol','threads')\n",
    "# for S2 in S1.find_all(recursive=False):\n",
    "#     if 'moved' not in S2['class']:\n",
    "#         title = S2.find('a','title').text.strip()\n",
    "#         post_addr = S2.find('a','title')['href'][len(base_url):-1].split('/')\n",
    "#         post_addr = '/'.join(post_addr[:-1])\n",
    "#         user_name = S2.find('a','username understate').text.strip()\n",
    "#         user_addr = S2.find('a','username understate')['href'].split('/')[-2]\n",
    "#         date_time = S2.find('a','username understate')['title'].split(' on ')[-1]\n",
    "#         scores = S2.find('ul','threadstats td alt').find_all(recursive=False)\n",
    "#         replies = int(scores[0].text.replace(',','').strip().split('\\t')[-1])\n",
    "#         views = int(scores[1].text.replace(',','').strip().split(' ')[-1])\n",
    "#         print(title)\n",
    "#         print(post_addr)\n",
    "#         print(user_name)\n",
    "#         print(user_addr)\n",
    "#         print(date_time)\n",
    "#         print(replies)\n",
    "#         print(views)\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crawler]",
   "language": "python",
   "name": "conda-env-crawler-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
